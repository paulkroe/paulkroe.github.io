<!DOCTYPE html>
<html lang="en-gb"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">Naive Bayes for Text Classification | Blog</title>
<meta property="og:title" content="Naive Bayes for Text Classification | Blog" />
<meta name="twitter:title" content="Naive Bayes for Text Classification | Blog" />
<meta itemprop="name" content="Naive Bayes for Text Classification | Blog" />
<meta name="application-name" content="Naive Bayes for Text Classification | Blog" />
<meta property="og:site_name" content="Paul&#39;s Log" />

<meta name="description" content="Using Naive Bayes for classifying  20 Newsgroups dataset.">
<meta itemprop="description" content="Using Naive Bayes for classifying  20 Newsgroups dataset." />
<meta property="og:description" content="Using Naive Bayes for classifying  20 Newsgroups dataset." />
<meta name="twitter:description" content="Using Naive Bayes for classifying  20 Newsgroups dataset." />

<meta property="og:locale" content="en-gb" />
<meta name="language" content="en-gb" />

  <link rel="alternate" hreflang="en-gb" href="http://localhost:1313/posts/naive-bayes/" title="English" />





    
    
    

    <meta property="og:type" content="article" />
    <meta property="og:article:published_time" content=2024-09-14T00:00:00Z />
    <meta property="article:published_time" content=2024-09-14T00:00:00Z />
    <meta property="og:url" content="http://localhost:1313/posts/naive-bayes/" />

    
    <meta property="og:article:author" content="Paul Kroeger" />
    <meta property="article:author" content="Paul Kroeger" />
    <meta name="author" content="Paul Kroeger" />
    
    

    

    <script defer type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "Article",
        "headline": "Naive Bayes for Text Classification",
        "author": {
        "@type": "Person",
        "name": ""
        },
        "datePublished": "2024-09-14",
        "description": "Using Naive Bayes for classifying  20 Newsgroups dataset.",
        "wordCount":  1106 ,
        "mainEntityOfPage": "True",
        "dateModified": "2024-09-14",
        "image": {
        "@type": "imageObject",
        "url": ""
        },
        "publisher": {
        "@type": "Organization",
        "name": "Blog"
        }
    }
    </script>


<meta name="generator" content="Hugo 0.134.1">

    
    <meta property="og:url" content="http://localhost:1313/posts/naive-bayes/">
  <meta property="og:site_name" content="Blog">
  <meta property="og:title" content="Naive Bayes for Text Classification">
  <meta property="og:description" content="Using Naive Bayes for classifying  20 Newsgroups dataset.">
  <meta property="og:locale" content="en_gb">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-09-14T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-09-14T00:00:00+00:00">


    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Naive Bayes for Text Classification">
  <meta name="twitter:description" content="Using Naive Bayes for classifying  20 Newsgroups dataset.">


    

    <link rel="canonical" href="http://localhost:1313/posts/naive-bayes/">
    <link href="/style.min.2d921c18cf1ec555ffc03d59a8adc211c402c68c930c27d6a0c306ab175a8d09.css" rel="stylesheet">
    <link href="/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="/favicon.ico">




<link rel="manifest" href="http://localhost:1313/site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">

    
    <link rel="icon" type="image/svg+xml" href="/icons/favicon.svg">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
    integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g==" crossorigin="anonymous" crossorigin="anonymous" />


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
    integrity="sha512-LQNxIMR5rXv7o+b1l8+N1EZMfhG7iFZ9HhnbJkTp4zjNr5Wvst75AqUeFDxeRUa7l5vEDyUiAip//r+EFLLCyA=="
    crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin="anonymous" onload="renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false}
      ]
    });"></script>
</head>
<body data-theme = "dark" class="notransition">

<script src="/js/theme.js"></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="http://localhost:1313/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title>Home</title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="/">
                        Home
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link active" href="/posts/">
                        Posts
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="/pages/about/">
                        About
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">Naive Bayes for Text Classification</h1>
                
                
                <div class="post-meta">
                    <time datetime="2024-09-14T00:00:00&#43;00:00" itemprop="datePublished"> 14 Sep 2024 </time>
                </div>
                
            </header>
            
    
    <details class="toc" ZgotmplZ>
        <summary><b>Table of Contents</b></summary>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#introduction-1">Introduction</a></li>
    <li><a href="#why-naive">Why Naive?</a></li>
    <li><a href="#why-bayes">Why Bayes?</a>
      <ul>
        <li><a href="#estimating-the-prior">Estimating the Prior</a></li>
        <li><a href="#estimating-the-likelihood">Estimating the Likelihood</a></li>
        <li><a href="#putting-everything-together">Putting Everything Together</a></li>
      </ul>
    </li>
    <li><a href="#final-details">Final Details</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
    </details>
            <div class="page-content">
                <h2 id="introduction">Introduction</h2>
<h2 id="introduction-1">Introduction</h2>
<p>Naive Bayes classifiers are simple yet powerful supervised learning algorithms commonly used for text classification tasks such as sentiment analysis, spam detection, language identification, and more. In this blog post, we will explore the theory behind Naive Bayes classifiers and implement one to categorize text in the <a href="http://qwone.com/~jason/20Newsgroups/">20 Newsgroups dataset</a>. When first learning about Naive Bayes, I found the book <a href="https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf"><em>Speech and Language Processing</em></a> by Daniel Jurafsky and James H. Martin extremely helpful.</p>
<h2 id="why-naive">Why Naive?</h2>
<p>Naive Bayes classifiers are probabilistic models that compute a probability distribution over possible classes for a given input document. Their beauty lies in their simplicity. However, natural language is notoriously complex, so to keep the approach manageable, we need to make some simplifying assumptions. Probably the most drastic simplification is in how input texts are represented. Naive Bayes models represent a text as a <strong>bag of words</strong>—that is, they consider each word (or token) in the input as part of an unordered collection, disregarding grammar and word order. Furthermore, unlike mathematical sets, this &ldquo;bag&rdquo; allows for multiple occurrences of the same word.</p>
<p>For example, if we use spaces to separate words (we will later ignore punctuation), the sentence “The Answer to the Great Question&hellip; Of Life, the Universe and Everything&hellip; Is&hellip; Forty-two” becomes the case-insensitive bag of words:</p>
<pre tabindex="0"><code>&#34;answer&#34;, &#34;forty-two&#34;, &#34;the&#34;, &#34;great&#34;, &#34;of&#34;, &#34;question&#34;, &#34;to&#34;, &#34;the&#34;, &#34;life&#34;, &#34;the&#34;, &#34;universe&#34;, &#34;is&#34;, &#34;everything&#34;, &#34;and&#34;
</code></pre><p>Note that the order of words is completely arbitrary, and words may appear multiple times.</p>
<p>This modeling choice has a significant implication: our model will not be able to incorporate sentence structure or the positions of words into its predictions. It relies solely on word frequencies.</p>
<h2 id="why-bayes">Why Bayes?</h2>
<p>Naive Bayes classifiers are probabilistic models that compute a probability distribution over possible classes for a given input document. Let $\mathcal{C}$ be the set of classes, and let $d$ be the document we want to classify. Our goal is to find the class $c \in \mathcal{C}$ that maximizes the posterior probability $P(c \mid d)$. Formally, we define:</p>
<p>$$
\hat{c} = \arg\max_{c \in \mathcal{C}} P(c \mid d)
$$</p>
<p>To estimate $P(c \mid d) we apply <strong>Bayes&rsquo; theorem</strong>. According to Bayes&rsquo; theorem:</p>
<p>$$
P(c \mid d) = \frac{P(d \mid c) P(c)}{P(d)}
$$</p>
<p>Substituting this into our classification rule, we get:</p>
<p>$$
\hat{c} = \arg\max_{c \in \mathcal{C}} \frac{P(d \mid c) P(c)}{P(d)} = \arg\max_{c \in \mathcal{C}} P(d \mid c) P(c)
$$</p>
<p>The second equality holds because $P(d)$ is constant with respect to $c$ and does not affect the maximization. Here, $P(d \mid c)$ is called <strong>likelihood</strong>, and $P(c)$ is refered to as <strong>prior</strong> probability of class $c$.</p>
<h3 id="estimating-the-prior">Estimating the Prior</h3>
<p>The prior $P(c)$ can be estimated using the relative frequency of class $c$ in the training set:</p>
<p>$$
\hat{P}(c) = \frac{\text{Number of documents in class } c}{\text{Total number of documents}}
$$</p>
<h3 id="estimating-the-likelihood">Estimating the Likelihood</h3>
<p>Estimating $P(d \mid c)$ directly is challenging due to the vast number of possible documents. To simplify, we make the <strong>Naive Bayes assumption</strong>: the features (words) are conditionally independent given the class. Representing our document $d$ as a sequence of words $w_1, w_2, \ldots, w_n$, this assumption gives the second equality below:</p>
<p>$$
P(d \mid c) = P(w_1, w_2, \ldots, w_n \mid c) = \prod_{w \in d} P(w \mid c)
$$</p>
<p>This assumption is strong and not generally true, as words in a document can be dependent (e.g., &ldquo;not&rdquo; and &ldquo;hungry&rdquo; in &ldquo;I am not hungry&rdquo;). However, since we&rsquo;re using a bag-of-words representation that ignores word order and syntax anyways, this simplification will turn out to be acceptable and it makes computation feasible.</p>
<p>Each term $P(w_i \mid c)$ answers the question: <em>&ldquo;How likely is word $w_i$ to appear in a document of class $c$?&rdquo;</em> We can estimate this probability by calculating the relative frequency of $w_i$ in all documents of class $c$:</p>
<p>$$
\hat{P}(w_i \mid c) = \frac{\text{Count}(w_i, c)}{\sum_{w \in V} \text{Count}(w, c)}
$$</p>
<p>Here, $V$ denotes the <strong>vocabulary</strong>, the set of all distinct words in the training documents.</p>
<h3 id="putting-everything-together">Putting Everything Together</h3>
<p>Combining our estimates of the prior and likelihood, we arrive at:</p>
<p>$$
\hat{c} = \arg\max_{c \in \mathcal{C}} \hat{P}(c) \prod_{w \in d} \hat{P}(w \mid c)
$$</p>
<h2 id="final-details">Final Details</h2>
<p>There are some important details we need to address.</p>
<p>Looking at our last equation for $\hat{c}$, we have a potentially large product, which is inefficient to compute. Moreover, since the factors of the product are probabilities less than one, multiplying many of them can lead to numerical underflow. To mitigate this, we employ a common technique: we take the logarithm of the expression. The logarithm is a strictly increasing function, so it does not change the location of the maximum. This transformation allows us to convert the product into a sum, which is more computationally efficient and numerically stable. Therefore, we can rewrite our equation as:</p>
<p>$$
\hat{c} = \arg\max_{c \in \mathcal{C}} \hat{P}(c) \prod_{w \in d} \hat{P}(w \mid c) = \arg\max_{c \in \mathcal{C}} \left( \log \hat{P}(c) + \sum_{w \in d} \log \hat{P}(w \mid c) \right)
$$</p>
<p>However, taking the logarithm introduces another problem. Specifically, the estimated likelihood $\hat{P}(w \mid c) = \frac{\text{count}(w, c)}{\sum_{w&rsquo; \in V} \text{count}(w&rsquo;, c)}$ might be zero if $\text{count}(w, c)$ is zero (i.e., the word $w$ has not appeared in the training data for class $c$). Since the logarithm of zero is undefined, we need to ensure that our estimated likelihoods are never zero. Intuitively, while the absence of a word can make a class less likely, it should not make the probability of that class zero outright. To avoid this issue, we apply a technique called <strong>smoothing</strong>.</p>
<p>There are several smoothing techniques available, but for simplicity, we&rsquo;ll use <strong>Laplace smoothing</strong> (also known as add-one smoothing). We adjust our estimate of the likelihood as follows:</p>
<p>$$
\hat{P}(w \mid c) = \frac{\text{count}(w, c) + 1}{\sum_{w&rsquo; \in V} \left( \text{count}(w&rsquo;, c) + 1 \right)} = \frac{\text{count}(w, c) + 1}{\left( \sum_{w&rsquo; \in V} \text{count}(w&rsquo;, c) \right) + |V|}
$$</p>
<p>Here, $|V|$ is the size of the vocabulary, i.e., the number of unique words in our training data. By adding 1 to each count in the numerator and adding $|V|$ to the denominator, we ensure that no probability estimate is zero.</p>
<p>Finally, we need to handle words in the test documents that are not present in our vocabulary (i.e., words we haven&rsquo;t seen during training). Since we cannot assign probabilities to these unseen words, our solution is to ignore them during classification. That is, we only consider words that are in our vocabulary when computing the sum of log-likelihoods.</p>
<p>This approach ensures that all terms in our calculation are well-defined and that our model can robustly handle new data.</p>
<h2 id="references">References</h2>
<p><a href="https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf">Speech and Language Processing, Daniel Jurafsky, James H. Martin</a>, Chapter 4</p>

            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
<a href="https://github.com/paulkroe" target="_blank" rel="noopener noreferrer me"
    title="Github">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
</a>
<a href="https://www.linkedin.com/in/paul-kroeger/" target="_blank" rel="noopener noreferrer me"
    title="Linkedin">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
</a>
</div>
    <small class="footer_copyright">
        © 2024 Paul Kroeger.
        Powered by <a href="https://github.com/hugo-sid/hugo-blog-awesome" target="_blank" rel="noopener">Hugo blog awesome</a>.
    </small>
</footer><a href="#" title="Go to top" id="totop">
    <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" fill="currentColor" stroke="currentColor" viewBox="0 96 960 960">
    <path d="M283 704.739 234.261 656 480 410.261 725.739 656 677 704.739l-197-197-197 197Z"/>
</svg>

</a>


    




    
    
        
    

    
    
        
    



    
    <script async src="http://localhost:1313/js/main.js" ></script>

    

</body>
</html>
